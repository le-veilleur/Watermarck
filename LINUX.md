# Linux & OS â€” Sous le capot d'un serveur haute performance

## Table des matiÃ¨res

1. [Pourquoi comprendre l'OS ?](#pourquoi)
2. [Le problÃ¨me C10K](#c10k)
3. [epoll â€” multiplexage d'I/O](#epoll)
4. [io_uring â€” I/O asynchrone sans syscall](#io_uring)
5. [Zero-copy â€” sendfile et splice](#zero-copy)
6. [mmap â€” fichiers en mÃ©moire virtuelle](#mmap)
7. [HiÃ©rarchie des caches CPU](#cpu-cache)
8. [NUMA â€” accÃ¨s mÃ©moire non-uniforme](#numa)
9. [Appels systÃ¨me (syscall) â€” le coÃ»t cachÃ©](#syscall)
10. [Processus vs Thread vs Goroutine](#goroutine)
11. [Docker FROM scratch â€” image minimale](#scratch)
12. [Primitives Linux dans notre projet](#projet)
13. [Outils de diagnostic](#outils)

---

<a name="pourquoi"></a>
## 1. Pourquoi comprendre l'OS ?

Un serveur Go tourne sur Linux. Quand tu appelles `http.ListenAndServe`, Go appelle l'OS, qui appelle le noyau, qui appelle le matÃ©riel.

```
Application Go
   â”‚
   â–¼ syscall (read/write/accept/epoll_wait...)
Noyau Linux (kernel)
   â”‚
   â–¼ pilotes (drivers)
MatÃ©riel (NIC, disque, RAM, CPU)
```

Comprendre ce qui se passe sous `net/http` explique :
- pourquoi un serveur plante Ã  1000 connexions simultanÃ©es
- pourquoi `sendfile` est 3x plus rapide que `read` + `write`
- pourquoi 8 goroutines â‰  8 threads OS

---

<a name="c10k"></a>
## 2. Le problÃ¨me C10K

**C10K = 10 000 connexions simultanÃ©es sur un seul serveur**

PosÃ© en 1999 par Dan Kegel. Ã€ l'Ã©poque, un serveur Apache crÃ©ait **1 thread par connexion**.

### L'approche "1 thread par connexion" (Apache 2.0)

```
Client 1  â”€â”€â–º Thread 1  (2 MB de stack par dÃ©faut)
Client 2  â”€â”€â–º Thread 2  (2 MB)
Client 3  â”€â”€â–º Thread 3  (2 MB)
...
Client 10 000 â”€â”€â–º Thread 10 000
```

**CoÃ»t en RAM :**
```
10 000 threads Ã— 2 MB = 20 GB de RAM rien que pour les stacks ğŸ’€
```

**CoÃ»t en CPU (context switching) :**
```
Le noyau doit "tourner" entre 10 000 threads
Chaque switch : sauvegarder les registres CPU, vider le TLB, recharger
â†’ des centaines de millisecondes perdues en overhead
```

### La solution : I/O multiplexÃ©e

Au lieu d'un thread par connexion, un seul thread surveille **N connexions** et rÃ©agit quand l'une d'elles est prÃªte.

```
1 thread
  â”‚
  â”œâ”€â”€ surveille connexion 1, 2, 3, ..., 10 000
  â”‚
  â””â”€â”€ quand connexion 42 a des donnÃ©es â†’ traite connexion 42
      quand connexion 1337 a des donnÃ©es â†’ traite connexion 1337
      ...
```

C'est le modÃ¨le de **nginx** et de **Go net/http**.

---

<a name="epoll"></a>
## 3. epoll â€” multiplexage d'I/O

`epoll` est le mÃ©canisme Linux (depuis 2.5.44, 2002) pour surveiller des milliers de file descriptors (fd) avec un seul thread.

### Ã‰volution : select â†’ poll â†’ epoll

#### select (1983, BSD)
```c
fd_set fds;
FD_SET(fd1, &fds);
FD_SET(fd2, &fds);
select(max_fd + 1, &fds, NULL, NULL, &timeout);
// ProblÃ¨me : copie tout le set en espace noyau O(n)
// Limite : FD_SETSIZE = 1024 file descriptors max
```

#### poll (1997)
```c
struct pollfd fds[10000];
fds[0].fd = fd1;
poll(fds, 10000, timeout);
// ProblÃ¨me : toujours O(n) Ã  chaque appel
// Pas de limite sur le nombre de fd
```

#### epoll (2002) â€” O(1) par Ã©vÃ©nement
```c
// â‘  CrÃ©er une instance epoll (1 seule fois)
int epfd = epoll_create1(0);

// â‘¡ Enregistrer un fd Ã  surveiller (1 seule fois par fd)
struct epoll_event ev;
ev.events = EPOLLIN;      // surveiller les donnÃ©es en entrÃ©e
ev.data.fd = client_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, client_fd, &ev);

// â‘¢ Attendre des Ã©vÃ©nements (bloque jusqu'Ã  activitÃ©)
struct epoll_event events[64];
int n = epoll_wait(epfd, events, 64, -1);

// â‘£ Traiter les fd prÃªts
for (int i = 0; i < n; i++) {
    handle(events[i].data.fd);  // seulement les fd actifs
}
```

### Comment epoll est O(1)

```
select/poll : "parmi ces 10 000 fd, lesquels sont prÃªts ?"
â†’ le noyau scanne TOUS les 10 000 Ã  chaque appel

epoll : le noyau maintient une liste interne (red-black tree)
â†’ quand un fd devient prÃªt, il est ajoutÃ© Ã  une file d'attente
â†’ epoll_wait retourne UNIQUEMENT les fd prÃªts
```

```
ComplexitÃ© select  : O(n) par appel
ComplexitÃ© epoll   : O(1) par Ã©vÃ©nement
                     O(log n) pour epoll_ctl (arbre rouge-noir)
```

### Go utilise epoll en coulisses

```go
// Ce code Go...
conn, err := listener.Accept()
go handleConn(conn)

// ...fait en coulisses :
// epoll_create1() au dÃ©marrage du runtime
// epoll_ctl(ADD, conn.fd)  quand Accept() retourne
// epoll_wait() dans le netpoller goroutine
// quand conn.fd est prÃªt â†’ rÃ©veille la goroutine en attente sur Read()
```

Le **runtime Go** a un netpoller qui fait tourner epoll en arriÃ¨re-plan. Chaque goroutine bloquÃ©e sur `conn.Read()` n'est **pas** bloquÃ©e dans le noyau â€” elle est parkÃ©e par le scheduler Go, et rÃ©veillÃ©e quand epoll signale que des donnÃ©es sont disponibles.

### EPOLLET â€” mode Edge-Triggered

```c
ev.events = EPOLLIN | EPOLLET;  // Edge-Triggered
// vs
ev.events = EPOLLIN;             // Level-Triggered (dÃ©faut)
```

| Mode | Comportement | Usage |
|------|--------------|-------|
| Level-Triggered | epoll_wait notifie **tant que** le fd est prÃªt | Plus simple, dÃ©faut |
| Edge-Triggered | epoll_wait notifie **une seule fois** quand le fd devient prÃªt | nginx, performances max |

Edge-Triggered force Ã  tout lire d'un coup â†’ moins d'appels epoll_wait â†’ plus rapide.

---

<a name="io_uring"></a>
## 4. io_uring â€” I/O asynchrone sans syscall

`io_uring` est le mÃ©canisme d'I/O asynchrone Linux depuis 5.1 (2019), conÃ§u par Jens Axboe.

### Le problÃ¨me des syscalls

Chaque opÃ©ration I/O coÃ»te un **changement de contexte** (user space â†’ kernel space) :

```
read(fd, buf, n)
  â”‚
  â”œâ”€â”€ sauvegarde registres CPU   (~100 ns)
  â”œâ”€â”€ switch vers kernel mode
  â”œâ”€â”€ vÃ©rifie permissions, copie buffer
  â”œâ”€â”€ switch vers user mode
  â””â”€â”€ restore registres CPU      (~100 ns)
```

Pour 1 million d'opÃ©rations I/O par seconde : `1M Ã— 200ns = 200ms` perdus en overhead.

### L'approche io_uring : ring buffers partagÃ©s

io_uring crÃ©e deux **ring buffers** partagÃ©s entre l'application et le noyau :

```
User space          Kernel space
     â”‚                    â”‚
     â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Submission Queue (SQ)               â”‚
â”‚  [op:read, fd:5, buf:0x...] â†â”€â”€ user   â”‚
â”‚  [op:write, fd:3, buf:0x...]            â”‚
â”‚  [op:accept, fd:1, ...]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Completion Queue (CQ)               â”‚
â”‚  [result: 1024 bytes read]  â”€â”€â–º user   â”‚
â”‚  [result: 512 bytes written]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sans io_uring :**
```
Pour 1000 opÃ©rations = 1000 syscalls = 1000 context switches
```

**Avec io_uring :**
```
â‘  Remplir le SQ avec 1000 opÃ©rations (en user space, pas de syscall)
â‘¡ io_uring_enter(1) â€” UN seul syscall pour soumettre tout
â‘¢ Le noyau exÃ©cute les 1000 opÃ©rations
â‘£ Lire les rÃ©sultats dans le CQ (en user space, pas de syscall)
```

### Modes d'opÃ©ration

```
Mode 1 : interruptible
  â†’ io_uring_enter() soumet + attend â†’ comme epoll mais batched

Mode 2 : SQPOLL (sans syscall du tout)
  â†’ thread kernel tourne en boucle et consomme le SQ
  â†’ l'application Ã©crit dans le SQ, le noyau lit automatiquement
  â†’ 0 syscall pour des milliers d'opÃ©rations
```

### OpÃ©rations supportÃ©es

```
RÃ©seau  : accept, recv, send, connect, sendmsg, recvmsg
Fichier : read, write, fsync, fallocate, rename, open
Timer   : timeout, link_timeout
Divers  : poll_add, cancel, provide_buffers
```

### io_uring en Go

```go
// Pas encore dans la stdlib, bibliothÃ¨ques tierces :
// github.com/iceber/iouring-go
// github.com/pawelgaczynski/giouring
// github.com/dshulyak/uring

req := iouring.Read(fd, buf, 0)
result, err := ring.SubmitAndWait(req)
```

**Gains typiques :**
- Redis avec io_uring : +20% de throughput
- Nginx avec io_uring : +40% en zero-copy file serving
- fio benchmark : 3x plus d'IOPS vs epoll pour des petits fichiers

---

<a name="zero-copy"></a>
## 5. Zero-copy â€” sendfile et splice

Quand un serveur sert un fichier statique, l'approche naÃ¯ve fait **4 copies** :

```
Disque â”€â”€â–º (1) Kernel buffer â”€â”€â–º (2) User buffer
       â”€â”€â–º (3) Kernel socket â”€â”€â–º (4) NIC (carte rÃ©seau)

Appels : read(fd, buf) â†’ write(sockfd, buf)
Copies : disque â†’ kernel RAM â†’ user RAM â†’ kernel socket â†’ NIC
```

C'est du gaspillage : 2 copies inutiles (vers user space et retour).

### sendfile â€” 2 copies seulement

```c
// Copie directement du fd fichier vers le fd socket
sendfile(sockfd, filefd, &offset, count);
```

```
Disque â”€â”€â–º (1) Kernel buffer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (2) NIC
                              DMA transfer

Appels : sendfile(sockfd, filefd, ...)
Copies : disque â†’ kernel RAM â†’ NIC
         (plus de passage par user space)
```

**En Go :**
```go
// http.ServeFile et os.File.WriteTo utilisent sendfile automatiquement
// quand la source est un *os.File et la destination est une *net.TCPConn

src, _ := os.Open("image.jpg")
dst, _ := net.Dial("tcp", "client:port")

// Go dÃ©tecte src=*os.File + dst=*net.TCPConn â†’ appelle sendfile(2) automatiquement
io.Copy(dst, src)
```

**nginx avec sendfile :**
```nginx
sendfile on;           # active sendfile(2)
tcp_nopush on;         # regroupe headers + dÃ©but du fichier (TCP_CORK)
tcp_nodelay on;        # dÃ©sactive Nagle pour les petits paquets finaux
```

### splice â€” entre deux fd quelconques

```c
// Copie entre deux fd sans passer par user space
// (pas forcÃ©ment des fichiers, peut Ãªtre pipes, sockets...)
splice(fd_in, NULL, fd_out, NULL, count, SPLICE_F_MOVE);
```

```
Socket in â”€â”€â–º (1) Kernel buffer â”€â”€â–º Socket out
             (rÃ©fÃ©rence kernel-to-kernel, pas de copie mÃ©moire)
```

### Tableau comparatif

| MÃ©thode | Copies mÃ©moire | Syscalls | Meilleur pour |
|---------|---------------|----------|---------------|
| `read` + `write` | 4 | 2 | Cas gÃ©nÃ©ral |
| `sendfile` | 2 | 1 | Fichier â†’ socket |
| `splice` | 0â€“2 | 1 | Pipe â†’ socket, socket â†’ socket |
| `io_uring` + `FIXED_BUFFERS` | 1 | ~0 | I/O intensive |

---

<a name="mmap"></a>
## 6. mmap â€” fichiers en mÃ©moire virtuelle

`mmap` mappe un fichier directement dans l'espace d'adressage du processus.

```c
void *addr = mmap(NULL, file_size, PROT_READ, MAP_SHARED, fd, 0);
// Maintenant addr[0..file_size] pointe directement sur le fichier
```

### Comment Ã§a fonctionne

```
MÃ©moire virtuelle du processus
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  code                             â”‚
  â”‚  heap                             â”‚
  â”‚  ...                              â”‚
  â”‚  0x7f3a00000000 â”€â”€â–º fichier.jpg  â”‚  â† mmap
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Noyau Linux
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Page Cache                       â”‚
  â”‚  fichier.jpg en RAM               â”‚ â†â”€â”€â”€ mÃªme page physique
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ page fault si pas encore chargÃ©e
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Disque          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

La premiÃ¨re lecture dÃ©clenche un **page fault** â†’ le noyau charge la page du disque dans le Page Cache â†’ la mappe dans le processus.

### Avantages

1. **ZÃ©ro copie** : les donnÃ©es du fichier ne sont jamais copiÃ©es en user space
2. **Lazy loading** : seules les pages rÃ©ellement lues sont chargÃ©es
3. **Partage** : plusieurs processus peuvent mapper le mÃªme fichier â†’ partage mÃ©moire

### Cas d'usage en serveurs

```go
// PostgreSQL, SQLite, RocksDB utilisent mmap pour leurs fichiers de donnÃ©es

// En Go avec golang.org/x/exp/mmap :
r, _ := mmap.Open("large_file.bin")
buf := make([]byte, 1024)
r.ReadAt(buf, offset)  // lecture sans copie vers kernel
```

### Quand NE PAS utiliser mmap

- **Fichiers changeants** : cohÃ©rence cache complexe
- **Petits fichiers** : overhead de mmap > bÃ©nÃ©fice
- **Random write** : amplification d'Ã©criture

---

<a name="cpu-cache"></a>
## 7. HiÃ©rarchie des caches CPU

Un accÃ¨s RAM prend ~100 ns. Un calcul CPU prend ~0.3 ns. Sans cache, le CPU passerait 99% du temps Ã  attendre la RAM.

### La hiÃ©rarchie

```
                    CPU (3.5 GHz)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Registres           â”‚  0.3 ns    ~100 bytes â”‚  (dans le processeur)
L1 Cache (data)     â”‚  1 ns       32 KB     â”‚  (par cÅ“ur)
L1 Cache (instr)    â”‚  1 ns       32 KB     â”‚  (par cÅ“ur)
L2 Cache            â”‚  4 ns      256 KB     â”‚  (par cÅ“ur)
L3 Cache (LLC)      â”‚  10 ns     8â€“32 MB    â”‚  (partagÃ© entre cÅ“urs)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
RAM (DRAM)                100 ns    GBâ€“TB
Disque SSD              100 000 ns  TB
```

### Cache line â€” l'unitÃ© de transfert

Le CPU ne transfÃ¨re pas octet par octet, mais par blocs de **64 bytes** (cache line).

```go
// Exemple : parcourir un tableau de structs

type Data struct {
    X int64  // 8 bytes
    Y int64  // 8 bytes
}

// âŒ False sharing : X et Y dans la mÃªme cache line
// Si goroutine 1 Ã©crit X et goroutine 2 Ã©crit Y simultanÃ©ment
// â†’ elles invalident mutuellement la cache line de l'autre
// â†’ performance similaire Ã  un mutex !

// âœ… Padding pour Ã©viter le false sharing
type DataPadded struct {
    X   int64
    _   [56]byte  // padding jusqu'Ã  64 bytes
    Y   int64
    _   [56]byte
}
```

### LocalitÃ© spatiale et temporelle

```go
// âŒ MAUVAISE localitÃ© spatiale : sauts mÃ©moire
for i := 0; i < n; i++ {
    sum += matrix[i][0]  // saute de ligne en ligne (non-contigu en mÃ©moire)
}

// âœ… BONNE localitÃ© spatiale : accÃ¨s sÃ©quentiels
for i := 0; i < n; i++ {
    for j := 0; j < m; j++ {
        sum += matrix[i][j]  // ligne par ligne = sÃ©quentiel en mÃ©moire
    }
}
```

**Pour un serveur d'images :** parcourir les pixels ligne par ligne est toujours plus rapide que colonne par colonne, car les images sont stockÃ©es row-major.

### PrÃ©chargement (prefetching)

Le CPU dÃ©tecte les accÃ¨s sÃ©quentiels et prÃ©charge les cache lines Ã  l'avance. Les accÃ¨s alÃ©atoires (linked lists, maps) ne bÃ©nÃ©ficient pas du prefetching.

```
Array (sequential) :  1 ns/op  (prefetch actif)
Linked list         : 50 ns/op  (pointer chasing, cache miss Ã  chaque nÅ“ud)
```

---

<a name="numa"></a>
## 8. NUMA â€” accÃ¨s mÃ©moire non-uniforme

Sur les serveurs multi-socket, chaque CPU a sa propre RAM locale.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Socket 0             â”‚    â”‚  Socket 1             â”‚
â”‚                       â”‚    â”‚                       â”‚
â”‚  CPU 0â€“15             â”‚    â”‚  CPU 16â€“31            â”‚
â”‚  RAM 0 : 64 GB        â”‚â—„â”€â”€â–ºâ”‚  RAM 1 : 64 GB        â”‚
â”‚  (accÃ¨s local : 4 ns) â”‚    â”‚ (accÃ¨s distant: 40 ns)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         QPI / UPI interconnect
```

**AccÃ¨s NUMA distant = 10x plus lent** que l'accÃ¨s local.

```bash
# Afficher la topologie NUMA
numactl --hardware

# Forcer un processus sur le nÅ“ud NUMA 0 (RAM et CPU locaux)
numactl --cpunodebind=0 --membind=0 ./server

# Voir les statistiques NUMA
numastat
```

**Pour Docker :** sur un serveur NUMA, affecter les containers Ã  un seul nÅ“ud NUMA Ã©vite les accÃ¨s distants coÃ»teux.

```bash
# Affecter un container au nÅ“ud NUMA 0
docker run --cpuset-cpus="0-15" --cpuset-mems="0" myapp
```

---

<a name="syscall"></a>
## 9. Appels systÃ¨me (syscall) â€” le coÃ»t cachÃ©

Un syscall traverse la frontiÃ¨re user space â†” kernel space.

### CoÃ»t d'un syscall

```
Avant Spectre/Meltdown (2017) : ~100 ns
AprÃ¨s patches KPTI             : ~200â€“400 ns
```

**Les patches Spectre forcent un vidage du TLB** Ã  chaque transition userâ†”kernel, ce qui multiplie le coÃ»t par 2â€“4.

### Syscalls courants dans un serveur HTTP

```
accept4()      â† nouvelle connexion TCP
read() / recv()â† donnÃ©es du client
write() / send() â† rÃ©ponse
close()        â† fermeture connexion
epoll_wait()   â† attente d'Ã©vÃ©nements
```

### Voir les syscalls d'un processus

```bash
# Trace tous les syscalls du processus 1234
strace -p 1234

# Compter les syscalls par type
strace -c ./server 2>&1

# Exemples de sortie :
% time     seconds  usecs/call     calls    syscall
 45.12    0.042341         42      1003    epoll_wait
 20.33    0.019089         19      1003    read
 15.22    0.014291         14      1003    write
  9.89    0.009281          9      1003    sendfile
  ...
```

### Batching pour rÃ©duire les syscalls

```go
// âŒ Ã‰criture octet par octet = des milliers de syscalls
for _, b := range data {
    conn.Write([]byte{b})
}

// âœ… Ã‰criture en une fois = 1 syscall
conn.Write(data)

// âœ… bufio.Writer regroupe les petites Ã©critures
bw := bufio.NewWriterSize(conn, 65536)
for _, line := range lines {
    bw.WriteString(line)
}
bw.Flush()  // 1 seul syscall
```

---

<a name="goroutine"></a>
## 10. Processus vs Thread vs Goroutine

```
Processus
  â”œâ”€â”€ espace mÃ©moire isolÃ© (4 GB virtual min)
  â”œâ”€â”€ crÃ©ation : fork() â‰ˆ 1 ms
  â”œâ”€â”€ context switch : â‰ˆ 1 Âµs + vidage TLB
  â””â”€â”€ communication : IPC, socket, pipe

Thread OS (pthread)
  â”œâ”€â”€ partage la mÃ©moire du processus
  â”œâ”€â”€ stack : 2 MB par dÃ©faut (configurable)
  â”œâ”€â”€ crÃ©ation : pthread_create â‰ˆ 10 Âµs
  â”œâ”€â”€ context switch : â‰ˆ 1 Âµs
  â””â”€â”€ limite pratique : ~10 000 threads

Goroutine Go
  â”œâ”€â”€ partage la mÃ©moire du processus
  â”œâ”€â”€ stack : 2â€“8 KB initial (grandit dynamiquement)
  â”œâ”€â”€ crÃ©ation : go f() â‰ˆ 300 ns
  â”œâ”€â”€ context switch : â‰ˆ 100 ns (gÃ©rÃ© par Go runtime)
  â””â”€â”€ limite pratique : ~1 000 000 goroutines
```

### Scheduler M:N de Go

Go utilise un scheduler **M:N** (M goroutines sur N threads OS) :

```
Goroutines (M)          Threads OS (N)          CPUs
   G1                      P0                   CPU0
   G2    â”€â”€â–º scheduler â”€â”€â–º P1  â”€â”€â–º threads â”€â”€â–º CPU1
   G3                      P2                   CPU2
   ...                     P3                   CPU3
   G1000

M = 1 000 000 goroutines
N = GOMAXPROCS (= nombre de cÅ“urs par dÃ©faut)
```

**P** = Processeur logique Go (chaque P tourne sur 1 thread OS)

Quand une goroutine fait un syscall bloquant (read, write), le scheduler dÃ©place les autres goroutines vers un autre P â†’ le thread ne reste pas bloquÃ©.

```bash
# Voir le scheduler en action
GODEBUG=schedtrace=1000 ./server
# Affiche l'Ã©tat du scheduler toutes les 1000ms
```

---

<a name="scratch"></a>
## 11. Docker FROM scratch â€” image minimale

**La question initiale : "le niveau zÃ©ro c'est faire un docker from scratch ?"**

RÃ©ponse : `FROM scratch` est une **image Docker vide**, pas une question de niveau 0 en Linux. C'est une optimisation de dÃ©ploiement. La vraie "base zÃ©ro" Linux, c'est le kernel et ses primitives (epoll, io_uring, sendfile...).

### Qu'est-ce que FROM scratch ?

```dockerfile
# Une image "normale"
FROM ubuntu:22.04     # â‰ˆ 77 MB (glibc, bash, apt, utils...)
COPY ./server /server
CMD ["/server"]

# FROM scratch = image complÃ¨tement vide
FROM scratch          # 0 bytes, rien
COPY ./server /server
CMD ["/server"]
```

`FROM scratch` = aucun OS, aucune libc, aucun shell. Juste ton binaire.

### Pourquoi c'est possible avec Go ?

Go compile en **binaire statique** qui n'a pas besoin de libc dynamique.

```bash
# Compilation statique Go (sans dÃ©pendances externes)
CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o server ./...

# VÃ©rifier qu'il n'y a aucune dÃ©pendance dynamique
ldd ./server
# â†’ not a dynamic executable âœ…
```

```bash
# Comparaison des tailles d'images Docker

FROM ubuntu    + server binary = 77 MB + 15 MB = 92 MB
FROM alpine    + server binary = 5 MB  + 15 MB = 20 MB
FROM distroless + server binary = 2 MB + 15 MB = 17 MB
FROM scratch   + server binary = 0 MB  + 15 MB = 15 MB
```

### Dockerfile multi-stage (pattern habituel)

```dockerfile
# â”€â”€ Stage 1 : Build â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM golang:1.23-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -ldflags="-s -w" -o server .
#              â†‘ pas de cgo     â†‘ linux     â†‘ strip debug symbols

# â”€â”€ Stage 2 : Image finale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM scratch

# Copier les certificats TLS (pour les appels HTTPS sortants)
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/

# Copier le binaire
COPY --from=builder /app/server /server

EXPOSE 3000
ENTRYPOINT ["/server"]
```

**`-ldflags="-s -w"` :**
- `-s` : supprime la table des symboles (debug)
- `-w` : supprime les infos DWARF (dÃ©bogage)
- RÃ©sultat : binaire 30â€“40% plus petit

### Ce que FROM scratch vous enlÃ¨ve

| Ce qu'on perd | Impact | Solution |
|---------------|--------|----------|
| Shell (`/bin/sh`) | Pas de `docker exec` interactif | Utiliser `docker cp` ou ajouter busybox |
| `/etc/passwd` | Pas d'utilisateur non-root par dÃ©faut | CrÃ©er l'utilisateur dans le builder, copier `/etc/passwd` |
| Certificats TLS | Appels HTTPS Ã©chouent | `COPY --from=builder /etc/ssl/certs/...` |
| Timezone data | `time.LoadLocation("Europe/Paris")` plante | `COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo` |
| Libresolv | DNS peut Ã©chouer (rare) | Utiliser `CGO_ENABLED=0` correctement |

### SÃ©curitÃ© de FROM scratch

```
Surface d'attaque :
  Ubuntu image : bash + apt + curl + python + 400+ packages â†’ 400+ vecteurs
  Scratch image : seulement votre binaire â†’ 1 vecteur
```

Un attaquant qui exploite une vulnÃ©rabilitÃ© dans votre serveur ne trouve **aucun outil** sur le systÃ¨me : pas de shell, pas de curl, pas de wget, rien.

```dockerfile
# Ajouter un utilisateur non-root pour encore plus de sÃ©curitÃ©
FROM scratch
COPY --from=builder /etc/passwd /etc/passwd
COPY --from=builder /app/server /server
USER nobody
ENTRYPOINT ["/server"]
```

### distroless â€” le compromis

`gcr.io/distroless/static` (Google) = scratch + certificats TLS + timezone + user nobody

```dockerfile
FROM gcr.io/distroless/static:nonroot
COPY --from=builder /app/server /server
ENTRYPOINT ["/server"]
```

Taille : ~2 MB. Plus facile Ã  utiliser que FROM scratch.

---

<a name="projet"></a>
## 12. Primitives Linux dans notre projet

Cartographie des syscalls utilisÃ©s par notre stack :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  front (nginx)                       â”‚
â”‚                                                     â”‚
â”‚  sendfile(2)     â† sert les fichiers statiques Reactâ”‚
â”‚  epoll_wait(2)   â† surveille les connexions clients â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  api (Go)                            â”‚
â”‚                                                     â”‚
â”‚  accept4(2)      â† nouvelles connexions HTTP        â”‚
â”‚  epoll_wait(2)   â† netpoller Go                     â”‚
â”‚  read(2)         â† lecture corps multipart          â”‚
â”‚  write(2)        â† Ã©criture rÃ©ponse JSON/image      â”‚
â”‚  futex(2)        â† sync.Mutex, channel operations   â”‚
â”‚  clone(2)        â† goroutines (via threads OS)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP (io.Pipe)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              optimizer (Go)                          â”‚
â”‚                                                     â”‚
â”‚  accept4(2)      â† connexions depuis l'API          â”‚
â”‚  read(2)         â† lecture image multipart          â”‚
â”‚  write(2)        â† Ã©criture image JPEG              â”‚
â”‚  madvise(2)      â† hints mÃ©moire pour image.RGBA    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Infrastructure :
  Redis  â† epoll, sendfile pour BGSAVE
  MinIO  â† sendfile pour objets disque â†’ socket
  RabbitMQ â† epoll, writev pour batching messages
```

### Voir les syscalls en pratique

```bash
# Tracer l'API pendant un upload
docker exec watermark-api-1 strace -p 1 -e trace=read,write,sendfile -c

# Compter les appels epoll
docker exec watermark-optimizer-1 strace -p 1 -e trace=epoll_wait -c 2>&1 | grep epoll

# Voir les file descriptors ouverts
ls -la /proc/$(pgrep server)/fd
```

---

<a name="outils"></a>
## 13. Outils de diagnostic Linux

### perf â€” profiling systÃ¨me

```bash
# Profiler les hot spots CPU de l'API
perf record -g -p $(pgrep api) -- sleep 30
perf report

# Compter les cache misses
perf stat -e cache-misses,cache-references ./server

# Flamegraph depuis perf
perf record -F 99 -g ./server &
sleep 30 && kill %1
perf script | stackcollapse-perf.pl | flamegraph.pl > flame.svg
```

### flamegraph avec Go pprof

```go
// Ajouter au serveur
import _ "net/http/pprof"

// http://localhost:6060/debug/pprof/goroutine
// http://localhost:6060/debug/pprof/heap
```

```bash
# GÃ©nÃ©rer un flamegraph
go tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile?seconds=30
```

### vmstat / iostat â€” vue d'ensemble

```bash
# I/O disque et CPU toutes les secondes
vmstat 1

# I/O par device
iostat -x 1

# MÃ©moire virtuelle et swapping
cat /proc/meminfo
```

### ss â€” connexions rÃ©seau (remplace netstat)

```bash
# Toutes les connexions TCP
ss -tnp

# Connexions en ESTABLISHED vers le port 3000
ss -tnp state established '( dport = :3000 or sport = :3000 )'

# Statistiques TCP
ss -s
```

### /proc â€” tout est un fichier

```bash
# Statistiques rÃ©seau du processus
cat /proc/$(pgrep api)/net/dev

# Consommation mÃ©moire dÃ©taillÃ©e
cat /proc/$(pgrep api)/status

# Syscalls depuis le dÃ©marrage
cat /proc/$(pgrep api)/syscall

# AffinitÃ© CPU
taskset -p $(pgrep api)
```

---

## ğŸ“Š RÃ©sumÃ©

| Primitive | ProblÃ¨me rÃ©solu | Gain |
|-----------|----------------|------|
| **epoll** | 1 thread peut gÃ©rer 100K connexions | scalabilitÃ© Ã—100 |
| **io_uring** | RÃ©duction des syscalls I/O | +20â€“40% throughput |
| **sendfile** | Copie fichier â†’ socket sans user space | Ã—2 vitesse serveur statique |
| **mmap** | Fichiers sans copie en user space | 0 copie, lazy load |
| **Cache CPU** | LocalitÃ© des donnÃ©es | Ã—10â€“50 selon algo |
| **FROM scratch** | Image Docker minimale | surface attaque Ã—0.01 |

---

## ğŸ”— Pour aller plus loin

- **"The Linux Programming Interface"** â€” Michael Kerrisk (bible de l'OS Linux)
- **"Systems Performance"** â€” Brendan Gregg (perf, flamegraph, Linux internals)
- `man 7 epoll`, `man 2 io_uring_setup`, `man 2 sendfile`
- **Linux kernel source** : `fs/read_write.c` (sendfile), `fs/io_uring.c`
- **"What every programmer should know about memory"** â€” Ulrich Drepper (CPU cache)

---

*"Un programme qui ne comprend pas l'OS qu'il tourne dessus laisse des performances sur la table."*
