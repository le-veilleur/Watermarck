# Cours : R√©silience
## Ne pas tomber sous charge ‚Äî Circuit Breaker, Rate Limiting, Retry, Context, Health Checks

---

## üìã Table des mati√®res

1. [C'est quoi la r√©silience ?](#intro)
2. [Circuit Breaker ‚Äî fail-fast](#circuit-breaker)
3. [Rate Limiting ‚Äî se prot√©ger des abus](#rate-limiting)
4. [Retry avec backoff exponentiel + jitter](#retry)
5. [context.Context ‚Äî annuler le travail inutile](#context)
6. [Health Checks ‚Äî readiness et liveness](#health)
7. [Graceful Shutdown ‚Äî z√©ro coupure](#shutdown)
8. [Bulkhead ‚Äî isoler les d√©faillances](#bulkhead)
9. [Timeout ‚Äî ne jamais attendre ind√©finiment](#timeout)
10. [Utilisation dans NWS Watermark](#watermark)
11. [R√©sum√© ‚Äî les patterns combin√©s](#r√©sum√©)

---

<a name="intro"></a>
## 1. C'est quoi la r√©silience ?

**R√©silience** = la capacit√© d'un syst√®me √† **d√©grader gracieusement** plut√¥t que s'effondrer brutalement quand quelque chose se passe mal.

**Analogie :** un disjoncteur √©lectrique. Quand il y a un court-circuit, il coupe le courant plut√¥t que de laisser br√ªler toute la maison. Il ne r√©pare pas la panne, mais il l'isole.

```
Syst√®me fragile (cascade failure) :
  Optimizer lent ‚Üí API attend ‚Üí toutes les goroutines bloquent ‚Üí RAM √©puis√©e ‚Üí API crashe ‚Üí front en erreur

Syst√®me r√©silient (degradation gracieuse) :
  Optimizer lent ‚Üí circuit breaker s'ouvre ‚Üí RabbitMQ prend le relais ‚Üí API r√©pond 202 ‚Üí front poll
```

### Les 8 fallacies des syst√®mes distribu√©s

Ces 8 hypoth√®ses que les d√©veloppeurs font √† tort en d√©butant :

1. Le r√©seau est fiable
2. La latence est nulle
3. La bande passante est infinie
4. Le r√©seau est s√©curis√©
5. La topologie ne change pas
6. Il y a un seul administrateur
7. Le co√ªt de transport est nul
8. Le r√©seau est homog√®ne

**Un syst√®me r√©silient accepte que ces hypoth√®ses soient fausses.**

---

<a name="circuit-breaker"></a>
## 2. Circuit Breaker ‚Äî fail-fast

### Le probl√®me sans circuit breaker

```
100 goroutines envoient des requ√™tes √† l'optimizer
L'optimizer est KO ‚Üí timeout √† 30s

Sans circuit breaker :
  ‚Üí 100 goroutines bloqu√©es 30s chacune
  ‚Üí 100 √ó 30s = 3000s de goroutines bloqu√©es
  ‚Üí chaque goroutine consomme ~8 KB de stack
  ‚Üí 100 √ó 8 KB = 800 KB minimum (+ les buffers)
  ‚Üí si les requ√™tes continuent d'arriver ‚Üí m√©moire √©puis√©e ‚Üí API crashe
```

### Les 3 √©tats

```
          5 √©checs cons√©cutifs
CLOSED ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ OPEN
(normal)                        (court-circuit)
   ‚ñ≤                                ‚îÇ
   ‚îÇ    succ√®s                      ‚îÇ apr√®s 30 secondes
   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HALF-OPEN ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 (1 requ√™te test)
                      ‚îÇ
                      ‚îî‚îÄ‚ñ∫ √©chec ‚Üí retour OPEN
```

**CLOSED (ferm√©) :** le circuit laisse passer les requ√™tes. On compte les √©checs.

**OPEN (ouvert) :** le circuit bloque toutes les requ√™tes imm√©diatement sans m√™me essayer. On retourne une erreur ou on bascule sur un fallback.

**HALF-OPEN (semi-ouvert) :** apr√®s le timeout, on laisse passer une seule requ√™te test. Si elle r√©ussit ‚Üí CLOSED. Si elle √©choue ‚Üí OPEN.

### Impl√©mentation avec gobreaker

```go
import "github.com/sony/gobreaker"

var optimizerCB *gobreaker.CircuitBreaker

func initCircuitBreaker() {
    optimizerCB = gobreaker.NewCircuitBreaker(gobreaker.Settings{
        Name:        "optimizer",
        MaxRequests: 1,              // 1 requ√™te test en half-open
        Interval:    10 * time.Second, // r√©initialise les compteurs toutes les 10s en CLOSED
        Timeout:     30 * time.Second, // temps avant de passer en HALF-OPEN depuis OPEN

        // Conditions pour passer en OPEN
        ReadyToTrip: func(counts gobreaker.Counts) bool {
            // Ouvrir si > 5 √©checs cons√©cutifs ET > 60% de taux d'√©chec
            failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
            return counts.ConsecutiveFailures >= 5 || (counts.Requests >= 10 && failureRatio >= 0.6)
        },

        // Callback sur changement d'√©tat
        OnStateChange: func(name string, from, to gobreaker.State) {
            log.Warn().
                Str("breaker", name).
                Str("from", from.String()).
                Str("to", to.String()).
                Msg("circuit breaker state changed")
        },
    })
}

func sendToOptimizerWithBreaker(url, filename string, data []byte, wmText, wmPosition string) ([]byte, error) {
    result, err := optimizerCB.Execute(func() (interface{}, error) {
        return sendToOptimizer(url, filename, data, wmText, wmPosition)
    })

    if err == gobreaker.ErrOpenState {
        // Circuit ouvert ‚Üí fallback imm√©diat sans attendre le timeout
        return nil, fmt.Errorf("optimizer circuit open, using fallback")
    }
    if err != nil {
        return nil, err
    }
    return result.([]byte), nil
}
```

### Int√©gration dans handleUpload

```go
tOptimizer := time.Now()
result, err := sendToOptimizerWithBreaker(optimizerURL, header.Filename, data, wmText, wmPosition)
if err != nil {
    // Que ce soit un vrai √©chec ou le circuit ouvert ‚Üí m√™me fallback RabbitMQ
    log.Warn().Err(err).Msg("optimizer unavailable, queuing job")
    replyWithRetryJob(w, ctx, cacheKey, originalKey, header.Filename, wmText, wmPosition, start)
    return
}
```

### Ce qu'on √©vite avec le circuit breaker

```
Sans CB : 100 req √ó 30s timeout = 3000s de goroutines bloqu√©es
Avec CB : 5 req √©chouent ‚Üí CB ouvert ‚Üí les 95 suivantes : 0ms de blocage, fallback imm√©diat
```

---

<a name="rate-limiting"></a>
## 3. Rate Limiting ‚Äî se prot√©ger des abus

### Pourquoi limiter les requ√™tes ?

```
Sans rate limiting :
  Un client malveillant envoie 10 000 images/sec
  ‚Üí l'optimizer est satur√© pour tout le monde
  ‚Üí les autres clients re√ßoivent des timeouts

Avec rate limiting :
  Chaque IP limit√©e √† 10 req/sec
  ‚Üí le client malveillant re√ßoit des 429 Too Many Requests
  ‚Üí les autres clients ne voient rien
```

### Token Bucket ‚Äî l'algorithme

```
Bucket capacity = 10 tokens
Refill rate     = 2 tokens/sec

T=0s  : bucket = [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 10 tokens
  3 requ√™tes simultan√©es :
T=0s  : bucket = [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñë‚ñë‚ñë] 7 tokens  (-3)
T=1s  : bucket = [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñë] 9 tokens  (+2, cap 10)
T=1s  : 5 requ√™tes ‚Üí bucket = [‚ñ†‚ñ†‚ñ†‚ñ†‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 4 tokens  (-5)
T=2s  : bucket = [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñë‚ñë‚ñë‚ñë] 6 tokens  (+2)
...

‚Üí Bursts autoris√©s (jusqu'√† 10 req instantan√©es)
‚Üí D√©bit moyen limit√© √† 2 req/sec sur le long terme
```

### Impl√©mentation avec golang.org/x/time/rate

```go
import (
    "net/http"
    "sync"
    "time"
    "golang.org/x/time/rate"
)

// Un limiteur par IP
type ipLimiter struct {
    limiter  *rate.Limiter
    lastSeen time.Time
}

var (
    mu       sync.Mutex
    limiters = make(map[string]*ipLimiter)
)

func getLimiter(ip string) *rate.Limiter {
    mu.Lock()
    defer mu.Unlock()

    if l, ok := limiters[ip]; ok {
        l.lastSeen = time.Now()
        return l.limiter
    }

    // 10 req/sec, burst de 20
    l := &ipLimiter{
        limiter:  rate.NewLimiter(rate.Limit(10), 20),
        lastSeen: time.Now(),
    }
    limiters[ip] = l
    return l.limiter
}

// Nettoyer les entr√©es inactives depuis plus de 3 minutes
func cleanupLimiters() {
    for {
        time.Sleep(3 * time.Minute)
        mu.Lock()
        for ip, l := range limiters {
            if time.Since(l.lastSeen) > 3*time.Minute {
                delete(limiters, ip)
            }
        }
        mu.Unlock()
    }
}

// Middleware
func rateLimitMiddleware(next http.Handler) http.Handler {
    go cleanupLimiters()  // d√©marrer le nettoyage en arri√®re-plan

    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        ip := r.RemoteAddr
        limiter := getLimiter(ip)

        if !limiter.Allow() {
            // Indiquer au client combien de temps attendre
            w.Header().Set("Retry-After", "1")
            w.Header().Set("X-RateLimit-Limit", "10")
            http.Error(w, "Too Many Requests", http.StatusTooManyRequests)
            return
        }

        next.ServeHTTP(w, r)
    })
}
```

### Sliding Window Counter ‚Äî l'alternative plus pr√©cise

Token Bucket autorise des bursts. Sliding Window interdit les bursts :

```
Max 100 requ√™tes sur les 60 derni√®res secondes

T=0   : 100 requ√™tes arrivent ‚Üí toutes accept√©es (bucket = 100)
T=1s  : 1 requ√™te ‚Üí refus√©e (on est encore dans la fen√™tre)
T=60s : la fen√™tre glisse ‚Üí les premi√®res requ√™tes sortent de la fen√™tre
T=60s : 100 nouvelles requ√™tes ‚Üí accept√©es

Impl√©mentation Redis :
  ZADD ratelimit:{ip} {timestamp} {request_id}
  ZREMRANGEBYSCORE ratelimit:{ip} 0 {now - 60s}
  count = ZCARD ratelimit:{ip}
  if count > 100 ‚Üí refuser
  EXPIRE ratelimit:{ip} 60
```

---

<a name="retry"></a>
## 4. Retry avec backoff exponentiel + jitter

### Le probl√®me du retry fixe

```go
// ‚ùå Actuel dans processRetryJob
time.Sleep(10 * time.Second)
```

**Thundering Herd problem :**
```
T=0   : 1000 jobs √©chouent
T=10s : 1000 jobs r√©essaient SIMULTAN√âMENT ‚Üí pic de charge ‚Üí ils r√©√©chouent tous
T=20s : 1000 jobs r√©essaient SIMULTAN√âMENT ‚Üí pic de charge ‚Üí ...
‚Üí boucle infinie qui surcharge le service exactement quand il essaie de r√©cup√©rer
```

### Backoff exponentiel

```go
// Chaque tentative double le d√©lai d'attente
// Attempt 0 : 1s
// Attempt 1 : 2s
// Attempt 2 : 4s
// Attempt 3 : 8s
// Attempt 4 : 16s
// ...jusqu'au max de 5 minutes

func exponentialBackoff(attempt int) time.Duration {
    base := time.Second
    max  := 5 * time.Minute
    exp  := base << attempt  // base * 2^attempt
    if exp > max {
        exp = max
    }
    return exp
}
```

### Jitter ‚Äî disperser les retries dans le temps

Sans jitter, tous les jobs qui √©chouent ensemble r√©essaient ensemble ‚Üí m√™me pic de charge.
Le jitter ajoute une dur√©e al√©atoire pour disperser les retries.

```go
import "math/rand"

// Full jitter : d√©lai al√©atoire entre 0 et exp
func fullJitter(attempt int) time.Duration {
    exp := exponentialBackoff(attempt)
    return time.Duration(rand.Int63n(int64(exp)))
}

// Equal jitter : exp/2 + random(exp/2) ‚Üí garantit un minimum d'attente
func equalJitter(attempt int) time.Duration {
    exp := exponentialBackoff(attempt)
    half := exp / 2
    return half + time.Duration(rand.Int63n(int64(half)))
}
```

**R√©sultat avec equal jitter :**
```
1000 jobs √©chouent simultan√©ment
  ‚Üí Job 1   attend 2.3s
  ‚Üí Job 2   attend 1.8s
  ‚Üí Job 3   attend 3.1s
  ‚Üí ...
  ‚Üí les retries sont dispers√©s sur ~2s au lieu d'√™tre simultan√©s
  ‚Üí charge distribu√©e uniform√©ment ‚Üí le service r√©cup√®re tranquillement
```

### Impl√©mentation dans processRetryJob

```go
func processRetryJob(msg amqp.Delivery, optimizerURL string) {
    var job RetryJob
    if err := json.Unmarshal(msg.Body, &job); err != nil {
        msg.Ack(false)
        return
    }

    // R√©cup√©rer le num√©ro de tentative depuis les headers AMQP
    attempt := 0
    if headers := msg.Headers; headers != nil {
        if v, ok := headers["x-attempt"].(int32); ok {
            attempt = int(v)
        }
    }

    data, err := fetchFromMinio(job.OriginalKey)
    if err != nil {
        wait := equalJitter(attempt)
        log.Warn().Int("attempt", attempt).Dur("retry_in", wait).Msg("minio fetch failed, retrying")
        msg.Nack(false, true)
        time.Sleep(wait)
        return
    }

    result, err := sendToOptimizer(optimizerURL, job.Filename, data, job.WmText, job.WmPosition)
    if err != nil {
        if attempt >= 5 {
            // Trop de tentatives ‚Üí envoyer en DLQ
            log.Error().Int("attempt", attempt).Msg("max retries exceeded, sending to DLQ")
            msg.Nack(false, false)  // false = ne pas requeue ‚Üí ira en DLQ
            return
        }
        wait := equalJitter(attempt)
        log.Warn().Int("attempt", attempt).Dur("retry_in", wait).Msg("optimizer still KO")
        msg.Nack(false, true)
        time.Sleep(wait)
        return
    }

    redisClient.Set(context.Background(), job.Hash, result, 24*time.Hour)
    msg.Ack(false)
}
```

---

<a name="context"></a>
## 5. context.Context ‚Äî annuler le travail inutile

### Le probl√®me actuel

```
Client mobile upload une image ‚Üí perd sa connexion WiFi √† mi-chemin
‚Üí API continue de traiter (MinIO, Optimizer, Redis)
‚Üí 312ms de travail pour rien
‚Üí Redis.Set stocke un r√©sultat que personne ne lira jamais
‚Üí en prod, des milliers de ces requ√™tes orphelines gaspillent des ressources
```

### context.Context ‚Äî la solution Go

`context.Context` est une interface qui porte :
- Un **signal d'annulation** (la connexion client est coup√©e)
- Un **deadline** (maximum 10s pour r√©pondre)
- Des **valeurs** (request_id, user_id propag√©s)

```go
// Hi√©rarchie de contextes
ctx := context.Background()          // racine, jamais annul√©e

// Annulation manuelle
ctx, cancel := context.WithCancel(ctx)
defer cancel()  // annule tout le sous-arbre √† la fin

// Timeout global
ctx, cancel = context.WithTimeout(ctx, 10*time.Second)
defer cancel()

// Deadline absolue
ctx, cancel = context.WithDeadline(ctx, time.Now().Add(10*time.Second))
defer cancel()
```

### Propagation dans handleUpload

```go
func handleUpload(w http.ResponseWriter, r *http.Request) {
    // r.Context() est annul√© automatiquement si le client d√©connecte
    ctx := r.Context()

    // Timeout global pour toute la cha√Æne : 30 secondes max
    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
    defer cancel()

    // ‚ë† Redis respecte le contexte ‚Äî si client d√©connecte, Redis.Get s'arr√™te
    data, err := redisClient.Get(ctx, cacheKey).Bytes()
    if errors.Is(err, context.Canceled) {
        log.Info().Msg("client disconnected during redis lookup")
        return
    }

    // ‚ë° MinIO respecte le contexte
    _, err = minioClient.PutObject(ctx, minioBucket, originalKey, ...)
    if err != nil {
        if errors.Is(err, context.DeadlineExceeded) {
            log.Warn().Msg("minio put timeout after 30s")
        }
        return
    }

    // ‚ë¢ Timeout sp√©cifique pour l'optimizer : 10s max
    optCtx, optCancel := context.WithTimeout(ctx, 10*time.Second)
    defer optCancel()

    result, err := sendToOptimizerWithContext(optCtx, optimizerURL, ...)
    if errors.Is(err, context.DeadlineExceeded) {
        log.Warn().Msg("optimizer timeout after 10s, queuing job")
        replyWithRetryJob(w, ctx, ...)
        return
    }
}
```

### V√©rifier si le contexte est annul√© dans une boucle

```go
// Pour les op√©rations longues, v√©rifier p√©riodiquement
for _, chunk := range chunks {
    select {
    case <-ctx.Done():
        return ctx.Err()  // annulation ou timeout
    default:
    }
    processChunk(chunk)
}
```

### Propagation cross-service

```go
// Propager le contexte vers l'optimizer via HTTP
func sendToOptimizerWithContext(ctx context.Context, url, filename string, data []byte, ...) ([]byte, error) {
    req, err := http.NewRequestWithContext(ctx, "POST", url+"/optimize", pr)
    // Si ctx est annul√© ‚Üí la requ√™te HTTP est annul√©e automatiquement
    resp, err := httpClient.Do(req)
    ...
}
```

---

<a name="health"></a>
## 6. Health Checks ‚Äî readiness et liveness

### Les deux types de checks

**Liveness** : le processus est-il vivant ? (r√©pondre OUI ou ne pas r√©pondre)
‚Üí Si liveness √©choue ‚Üí Kubernetes red√©marre le pod

**Readiness** : le processus est-il pr√™t √† recevoir du trafic ?
‚Üí Si readiness √©choue ‚Üí Kubernetes retire le pod du load balancer (mais ne le red√©marre pas)

```
Liveness  : "je tourne"           ‚Üí vrai m√™me si Redis est KO
Readiness : "je peux servir"      ‚Üí faux si Redis ou MinIO est KO
```

### Impl√©mentation

```go
// /health ‚Äî liveness : juste "je suis vivant"
mux.HandleFunc("GET /health", func(w http.ResponseWriter, r *http.Request) {
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(map[string]string{"status": "ok"})
})

// /ready ‚Äî readiness : toutes les d√©pendances sont-elles disponibles ?
mux.HandleFunc("GET /ready", func(w http.ResponseWriter, r *http.Request) {
    ctx, cancel := context.WithTimeout(r.Context(), 2*time.Second)
    defer cancel()

    type check struct {
        name string
        err  error
    }
    checks := []check{
        {"redis", redisClient.Ping(ctx).Err()},
        {"minio", checkMinio(ctx)},
        {"rabbitmq", checkRabbitMQ()},
    }

    failed := map[string]string{}
    for _, c := range checks {
        if c.err != nil {
            failed[c.name] = c.err.Error()
        }
    }

    w.Header().Set("Content-Type", "application/json")
    if len(failed) > 0 {
        w.WriteHeader(http.StatusServiceUnavailable)
        json.NewEncoder(w).Encode(map[string]interface{}{
            "status": "not ready",
            "failed": failed,
        })
        return
    }

    json.NewEncoder(w).Encode(map[string]string{"status": "ready"})
})
```

### Configuration Docker Compose

```yaml
api:
  healthcheck:
    test: ["CMD", "wget", "-qO-", "http://localhost:3000/health"]
    interval: 10s
    timeout: 5s
    retries: 3
    start_period: 15s  # laisse le temps au service de d√©marrer
  depends_on:
    redis:
      condition: service_healthy
    minio:
      condition: service_healthy
    rabbitmq:
      condition: service_healthy
```

---

<a name="shutdown"></a>
## 7. Graceful Shutdown ‚Äî z√©ro coupure

### Le probl√®me d'un arr√™t brutal

```
docker stop ‚Üí SIGTERM ‚Üí processus tu√© imm√©diatement

‚Üí Requ√™te en cours (optimize, 300ms) ‚Üí perdue brutalement
‚Üí Client re√ßoit une erreur r√©seau au lieu d'une r√©ponse
‚Üí En rolling deployment : quelques secondes de 50x errors
```

### Graceful shutdown

```go
func main() {
    srv := &http.Server{
        Addr:    ":3000",
        Handler: corsMiddleware(mux),

        // Timeouts pour √©viter les connexions qui tra√Ænent
        ReadTimeout:  5 * time.Second,
        WriteTimeout: 60 * time.Second,
        IdleTimeout:  120 * time.Second,
    }

    // D√©marrer le serveur dans une goroutine
    go func() {
        if err := srv.ListenAndServe(); err != http.ErrServerClosed {
            log.Fatal().Err(err).Msg("server error")
        }
    }()

    log.Info().Str("addr", ":3000").Msg("server started")

    // Attendre SIGTERM (docker stop) ou SIGINT (Ctrl+C)
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGTERM, syscall.SIGINT)
    sig := <-quit

    log.Info().Str("signal", sig.String()).Msg("shutdown initiated")

    // 30 secondes pour finir les requ√™tes en cours
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    // Shutdown accepte les nouvelles connexions mais attend les existantes
    if err := srv.Shutdown(ctx); err != nil {
        log.Error().Err(err).Msg("forced shutdown after timeout")
    }

    log.Info().Msg("server stopped cleanly")
}
```

### Ordre d'arr√™t propre

```
1. srv.Shutdown() ‚Üí stop d'accepter de nouvelles connexions
2. Attendre que les handlers en cours finissent
3. Fermer la connexion RabbitMQ (apr√®s avoir NACK√© les messages en cours)
4. Fermer Redis (flush les derni√®res commandes)
5. log.Info().Msg("bye")
6. os.Exit(0)
```

---

<a name="bulkhead"></a>
## 8. Bulkhead ‚Äî isoler les d√©faillances

**Analogie :** les cloisons √©tanches d'un sous-marin. Si un compartiment est inond√©, les autres restent secs.

**Sur NWS Watermark :** s√©parer les ressources (goroutines, connexions) pour que les uploads lents n'impactent pas les lookups rapides.

```go
// Worker pool s√©par√© pour les uploads lents (optimizer)
var optimizerSem = make(chan struct{}, 10)  // max 10 uploads simultan√©s

// Worker pool s√©par√© pour les lookups rapides (Redis)
var redisSem = make(chan struct{}, 50)      // max 50 lookups simultan√©s

// Si l'optimizer est satur√© ‚Üí les lookups (cache HIT) ne sont pas impact√©s
func handleUpload(w http.ResponseWriter, r *http.Request) {
    // Acqu√©rir un slot optimizer (bloque si 10 uploads en cours)
    select {
    case optimizerSem <- struct{}{}:
        defer func() { <-optimizerSem }()
    case <-time.After(5 * time.Second):
        http.Error(w, "Service busy, try again", http.StatusServiceUnavailable)
        return
    }
    // ...
}
```

---

<a name="timeout"></a>
## 9. Timeout ‚Äî ne jamais attendre ind√©finiment

**R√®gle d'or :** chaque appel r√©seau doit avoir un timeout.

```go
// ‚úÖ Timeouts sur le client HTTP
var httpClient = &http.Client{
    Timeout: 30 * time.Second,  // timeout global

    Transport: &http.Transport{
        DialContext: (&net.Dialer{
            Timeout:   3 * time.Second,   // timeout connexion TCP
            KeepAlive: 30 * time.Second,
        }).DialContext,
        TLSHandshakeTimeout:   5 * time.Second,
        ResponseHeaderTimeout: 10 * time.Second,  // timeout pour recevoir les headers
        IdleConnTimeout:       90 * time.Second,
        MaxIdleConns:          100,               // pool de connexions
        MaxIdleConnsPerHost:   10,
    },
}
```

### Timeout par op√©ration

```
Op√©ration            Timeout raisonnable
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Connexion TCP        3s
TLS handshake        5s
Redis GET/SET        1s
MinIO PUT (2MB)      30s
Optimizer            15s
R√©ponse totale       60s
```

---

<a name="watermark"></a>
## 10. Utilisation dans NWS Watermark

### Ce qui manque actuellement

| Pattern | Statut | Impact si absent |
|---|---|---|
| Circuit Breaker | ‚ùå Manquant | Goroutines bloqu√©es 30s si optimizer KO |
| Rate Limiting | ‚ùå Manquant | Abus possible, saturation optimizer |
| Backoff exponentiel | ‚ùå Manquant | Thundering herd sur les retries |
| context propagation | ‚ùå Manquant | Travail inutile si client d√©connecte |
| Health checks | ‚ùå Manquant | Pas d'int√©gration Docker/Kubernetes |
| Graceful shutdown | ‚ùå Manquant | Requ√™tes perdues au d√©ploiement |
| Bulkhead | ‚úÖ Partiel | Worker pool c√¥t√© optimizer uniquement |
| Timeouts HTTP | ‚úÖ Partiel | `httpClient` a un timeout de 30s |

### Ordre d'impl√©mentation recommand√©

```
1. context + timeout    ‚Üí le plus impactant, annule le travail inutile
2. Graceful shutdown    ‚Üí z√©ro interruption au d√©ploiement
3. Health checks        ‚Üí int√©gration Docker Compose
4. Rate limiting        ‚Üí se prot√©ger des abus avant d'aller en prod
5. Circuit breaker      ‚Üí si l'optimizer est un service externe critique
6. Backoff exponentiel  ‚Üí am√©liorer les retries RabbitMQ
7. Bulkhead             ‚Üí isoler uploads lents des lookups rapides
```

---

<a name="r√©sum√©"></a>
## 11. R√©sum√© ‚Äî les patterns combin√©s

### Vue d'ensemble

```
Requ√™te entrante
      ‚îÇ
      ‚ñº
Rate Limiter ‚îÄ‚îÄ‚ñ∫ 429 si abus
      ‚îÇ
      ‚ñº
Context + Timeout (30s global)
      ‚îÇ
      ‚îú‚îÄ‚îÄ‚ñ∫ Redis (timeout 1s)
      ‚îÇ     ‚îî‚îÄ‚îÄ HIT ‚Üí r√©ponse directe
      ‚îÇ
      ‚îú‚îÄ‚îÄ‚ñ∫ MinIO (timeout 30s)
      ‚îÇ
      ‚îú‚îÄ‚îÄ‚ñ∫ Circuit Breaker ‚îÄ‚îÄ‚ñ∫ OPEN ‚Üí fallback RabbitMQ imm√©diat
      ‚îÇ         ‚îÇ
      ‚îÇ         ‚îî‚îÄ‚îÄ CLOSED ‚Üí Optimizer (timeout 10s)
      ‚îÇ
      ‚îî‚îÄ‚îÄ‚ñ∫ context.Done() ‚Üí annulation si client d√©connecte
```

### Les 4 patterns essentiels

| Pattern | Prot√®ge contre | Impl√©mentation Go |
|---|---|---|
| Circuit Breaker | Cascade failure | `sony/gobreaker` |
| Rate Limiting | Abus / surcharge | `golang.org/x/time/rate` |
| Context + Timeout | Travail inutile / attente infinie | `context.WithTimeout` |
| Graceful Shutdown | Perte de requ√™tes au d√©ploiement | `http.Server.Shutdown()` |

### R√®gles √† retenir

1. **Tout appel r√©seau a un timeout** ‚Äî jamais d'attente infinie
2. **Fail-fast > fail-slow** ‚Äî retourner une erreur rapidement vaut mieux que bloquer
3. **D√©gradation gracieuse** ‚Äî avoir toujours un fallback (RabbitMQ, 202, cache stale)
4. **Isoler les d√©faillances** ‚Äî une panne ne doit pas cascader
5. **Mesurer d'abord** ‚Äî ajouter le circuit breaker l√† o√π les pannes arrivent vraiment (pprof + Prometheus avant)
